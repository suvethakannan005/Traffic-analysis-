import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
# Load the dataset
file_path = ('accident.csv')
df = pd.read_csv(file_path)
# Preview data
print("Initial data preview:")
print(df.head())
print("\nSummary statistics:")
print(df.describe(include='all'))
# Check for missing values
print("\nMissing values per column:")
print(df.isnull().sum())
# Drop columns with too many missing values or irrelevant data (customize based on your dataset)
threshold = 0.5  # drop columns with more than 50% missing values
df = df.loc[:, df.isnull().mean() < threshold]
# Fill remaining missing values
for col in df.columns:
    if df[col].dtype == 'object':
        df[col].fillna(df[col].mode()[0], inplace=True)
    else:
        df[col].fillna(df[col].median(), inplace=True)
# Encode categorical features
label_encoders = {}
for col in df.select_dtypes(include='object').columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le
# Feature scaling
scaler = StandardScaler()
scaled_features = scaler.fit_transform(df)
df_scaled = pd.DataFrame(scaled_features, columns=df.columns)
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
# Load the dataset
file_path = ('accident.csv')
df = pd.read_csv(file_path)
# Preview data
print("Initial data preview:")
print(df.head())
print("\nSummary statistics:")
print(df.describe(include='all'))
# Check for missing values
print("\nMissing values per column:")
print(df.isnull().sum())
# Drop columns with too many missing values or irrelevant data (customize based on your dataset)
threshold = 0.5  # drop columns with more than 50% missing values
df = df.loc[:, df.isnull().mean() < threshold]
# Fill remaining missing values
for col in df.columns:
    if df[col].dtype == 'object':
        df[col].fillna(df[col].mode()[0], inplace=True)
    else:
        df[col].fillna(df[col].median(), inplace=True)
# Encode categorical features
label_encoders = {}
for col in df.select_dtypes(include='object').columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le
# Feature scaling
scaler = StandardScaler()
scaled_features = scaler.fit_transform(df)
df_scaled = pd.DataFrame(scaled_features, columns=df.columns)
# Define your target column (e.g., 'Accident_Severity', update based on your dataset)
target_column = 'Accident_Severity'  # Replace with actual column name from your dataset
if target_column in df_scaled.columns:
    X = df_scaled.drop(columns=target_column)
    y = df_scaled[target_column]
    # Split the dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    print("Data preparation complete.")
else:
    print(f"Error: Target column '{target_column}' not found in the scaled DataFrame.")
    print("Available columns after scaling:", df_scaled.columns.tolist())
# Importing necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
# Load the dataset
df = pd.read_csv('accident.csv')
# Display basic info
print("Shape of dataset:", df.shape)
print("\nColumns:\n", df.columns.tolist())
print("\nData Types:\n", df.dtypes)
print("\nFirst 5 Rows:\n", df.head())
print("\nMissing Values:\n", df.isnull().sum())
# Summary statistics
print("\nSummary Statistics:\n", df.describe(include='all'))
# Check for duplicate rows
print("\nNumber of duplicate rows:", df.duplicated().sum())
# Data cleaning idea: drop duplicates
df = df.drop_duplicates()
# Distribution of categorical variables
categorical_cols = df.select_dtypes(include='object').columns
for col in categorical_cols:
    print(f"\nValue counts for {col}:\n", df[col].value_counts())
# Correlation heatmap for numerical features
plt.figure(figsize=(10, 6))
sns.heatmap(df.select_dtypes(include=['float64', 'int64']).corr(), annot=True, cmap='coolwarm')
plt.title("Correlation Heatmap")
plt.show()
# Plot accidents per day/hour if datetime column exists
if 'Date' in df.columns or 'Time' in df.columns:
    if 'Date' in df.columns:
        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
        df['DayOfWeek'] = df['Date'].dt.day_name()
        df['Month'] = df['Date'].dt.month_name()
        df['Year'] = df['Date'].dt.year
        # Accidents by day of week
        plt.figure(figsize=(8, 5))
        sns.countplot(data=df, x='DayOfWeek', order=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])
        plt.title("Accidents by Day of the Week")
        plt.xticks(rotation=45)
        plt.show()
        # Accidents by month
        plt.figure(figsize=(10, 5))
        sns.countplot(data=df, x='Month')
        plt.title("Accidents by Month")
        plt.xticks(rotation=45)
        plt.show()
    if 'Time' in df.columns:
        df['Time'] = pd.to_datetime(df['Time'], format='%H:%M:%S', errors='coerce').dt.hour
        plt.figure(figsize=(10, 5))
        sns.histplot(df['Time'], bins=24, kde=False)
        plt.title("Accidents by Hour of Day")
        plt.xlabel("Hour")
        plt.ylabel("Number of Accidents")
        plt.show()
# Interactive Map (if Lat/Lon available)
if 'Latitude' in df.columns and 'Longitude' in df.columns:
    fig = px.scatter_mapbox(
        df,
        lat="Latitude",
        lon="Longitude",
        hover_data=df.columns,
        zoom=5,
        height=500,
        title="Traffic Accidents Map",
    )
    fig.update_layout(mapbox_style="open-street-map")
    fig.show()
# Countplot for severity, weather, road conditions if available
for col in ['Severity', 'Weather_Condition', 'Road_Condition']:
    if col in df.columns:
        plt.figure(figsize=(8, 5))
        sns.countplot(data=df, x=col, order=df[col].value_counts().index)
        plt.title(f"Distribution of {col}")
        plt.xticks(rotation=45)
        plt.show()
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
# Load the dataset
df = pd.read_csv('accident.csv')
# Basic info
print("Initial Data Overview:")
print(df.info())
print(df.head())
# -------------------------
# Feature Engineering Steps
# -------------------------
# 1. Convert date/time columns
if 'Start_Time' in df.columns:
    df['Start_Time'] = pd.to_datetime(df['Start_Time'])
    df['Hour'] = df['Start_Time'].dt.hour
    df['Day'] = df['Start_Time'].dt.day
    df['Weekday'] = df['Start_Time'].dt.weekday
    df['Month'] = df['Start_Time'].dt.month
    df['Year'] = df['Start_Time'].dt.year
else:
    # If Start_Time is missing, create placeholder columns to avoid errors later
    # Or, you could skip the creation of time-dependent features altogether
    # For this fix, we'll create None placeholders to allow the script to continue
    # but you might need a more robust strategy depending on your needs.
    print("Warning: 'Start_Time' column not found. Time-based features will not be created.")
    df['Hour'] = None
    df['Weekday'] = None
# 2. Calculate duration (if End_Time exists)
if 'End_Time' in df.columns:
    df['End_Time'] = pd.to_datetime(df['End_Time'])
    # Ensure 'Start_Time' exists before calculating duration
    if 'Start_Time' in df.columns and df['Start_Time'] is not None:
         df['Duration_Minutes'] = (df['End_Time'] - df['Start_Time']).dt.total_seconds() / 60
    else:
         print("Warning: Cannot calculate 'Duration_Minutes' as 'Start_Time' is missing.")
         df['Duration_Minutes'] = None # Or np.nan
# 3. Handle missing values
df.fillna({
    'Weather_Condition': 'Unknown',
    #'Number_of_injuries(mi)': df['Number_of_injuries(mi)'].median(),
    #'Wind_Speed(mph)': df['Wind_Speed(mph)'].median(),
    #'Humidity(%)': df['Humidity(%)'].median()
}, inplace=True)
# 4. Encode categorical variables
cat_features = ['Side', 'Weather_Condition', 'Wind_Direction', 'State', 'City']
for col in cat_features:
    if col in df.columns:
        # Ensure column is not entirely None or NaN before encoding
        if df[col].notna().any():
             df[col] = LabelEncoder().fit_transform(df[col].astype(str))
        else:
             print(f"Warning: Column '{col}' contains only missing values. Skipping encoding.")
# 5. Create binary features from descriptive columns (if available)
# Add checks to ensure 'Hour' and 'Weekday' exist and are not entirely None
if 'Hour' in df.columns and df['Hour'] is not None and df['Hour'].notna().any():
    df['Is_Daytime'] = df['Hour'].apply(lambda x: 1 if 6 <= x <= 18 else 0)
else:
     print("Warning: Cannot create 'Is_Daytime' as 'Hour' is missing or empty.")
     df['Is_Daytime'] = None # Or np.nan
if 'Weekday' in df.columns and df['Weekday'] is not None and df['Weekday'].notna().any():
    df['Weekend'] = df['Weekday'].apply(lambda x: 1 if x >= 5 else 0)
else:
     print("Warning: Cannot create 'Weekend' as 'Weekday' is missing or empty.")
     df['Weekend'] = None # Or np.nan
# 6. Geospatial feature engineering (distance from city center or clustering)
# Example only if Latitude and Longitude exist
if 'Start_Lat' in df.columns and 'Start_Lng' in df.columns:
    # Round coordinates to cluster accident zones
    # Ensure columns are not entirely None or NaN before rounding
    if df['Start_Lat'].notna().any() and df['Start_Lng'].notna().any():
        df['Lat_bin'] = df['Start_Lat'].round(1)
        df['Lng_bin'] = df['Start_Lng'].round(1)
    else:
         print("Warning: Cannot create Lat/Lng bins as 'Start_Lat' or 'Start_Lng' is missing or empty.")
         df['Lat_bin'] = None # Or np.nan
         df['Lng_bin'] = None # Or np.nan
# 7. Drop irrelevant or high cardinality columns if needed
drop_cols = ['ID', 'Source', 'Description', 'Number', 'Street', 'Zipcode', 'Country']
df.drop(columns=[col for col in drop_cols if col in df.columns], inplace=True)
# Final dataset preview
print("\nFeature-engineered data preview:")
print(df.head())
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, mean_squared_error, roc_curve
from sklearn.preprocessing import LabelEncoder, StandardScaler
import matplotlib.pyplot as plt
import numpy as np
# Load the dataset
df = pd.read_csv('accident.csv')
# Replace 'Accident_Severity' with the actual target column in your dataset
target_col = 'Accident_ID'  # Example
X = df.drop(columns=[target_col])
y = df[target_col]
# Encode categorical target if needed
if y.dtype == 'object':
    le = LabelEncoder()
    y = le.fit_transform(y)
# Handle categorical features if present
X = pd.get_dummies(X)
# Scale the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)
# Train a classifier
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)
# Predictions
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1] if len(np.unique(y)) == 2 else None  # Only for binary classification
# Evaluation
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='weighted')
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"Accuracy: {accuracy:.4f}"
print(f"F1-score: {f1:.4f}")
print(f"RMSE: {rmse:.4f}")
# ROC and AUC for binary classification
if y_prob is not None:
    auc = roc_auc_score(y_test, y_prob)
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    print(f"AUC: {auc:.4f}")
    # Plot ROC Curve
    plt.figure()
    plt.plot(fpr, tpr, label=f'AUC = {auc:.2f}')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend(loc='lower right')
    plt.grid(True)
    plt.show()
else:
    print("ROC AUC and curve not available for multi-class classification.")
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# ML libraries
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
# Load dataset
df = pd.read_csv('accident.csv')
# Display basic info
print("Dataset Info:")
print(df.info())
print("\nMissing Values:\n", df.isnull().sum())
# Drop rows/columns with too many missing values (optional)
# df.dropna(thresh=0.7*len(df), axis=1, inplace=True)
# Fill or drop missing values
df.dropna(inplace=True)  # simple handling; you can customize
# Encode categorical variables
label_encoders = {}
for col in df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le
# Define features and target
target_column = 'Accident_Severity'  # Replace with actual column name from your dataset
# Check if the target column exists in the dataframe after preprocessing
if target_column in df.columns:
    # Define features (X) and target (y) from the preprocessed dataframe
    X = df.drop(columns=target_column)
    y = df[target_column]
    # Split the dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    print("Data preparation complete.")
    # Feature scaling - Apply scaling *after* the train/test split
    scaler = StandardScaler()
    # Fit the scaler on the training data and transform both training and testing data
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)
    # Model training
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    # Predictions
    y_pred = model.predict(X_test)
    # Evaluation
    print("Classification Report:\n", classification_report(y_test, y_pred))
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
    # Feature importance
    importances = model.feature_importances_
    indices = np.argsort(importances)[::-1]
    # Use the original column names from X for plotting
    features = X.columns
    plt.figure(figsize=(12, 6))
    sns.barplot(x=importances[indices], y=features[indices])
    plt.title("Feature Importance")
    plt.show()
else:
    print(f"Error: Target column '{target_column}' not found in the DataFrame after preprocessing.")
    print("Available columns:", df.columns.tolist())
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
import gradio as gr
# Load data
df = pd.read_csv("accident.csv")
# Features and target
features = ['Number_of_Deaths', 'Number_of_Injuries', 'Speed_Limit', 'Accident_ID']
target = 'Accident_Occurred'  # change if different in your dataset
# Drop missing
df = df[features + [target]].dropna()
# Encode Accident_ID if it's categorical
if df['Accident_ID'].dtype == 'object':
    df['Accident_ID'] = df['Accident_ID'].astype('category').cat.codes
# Train/test split
X = df[features]
y = df[target]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Train model
model = RandomForestClassifier()
model.fit(X_train, y_train)
# Gradio function
def predict_accident(deaths, injuries, speed, accident_id):
    input_data = pd.DataFrame({
        'Number_of_Deaths': [deaths],
        'Number_of_Injuries': [injuries],
        'Speed_Limit': [speed],
        'Accident_ID': [accident_id]
    })
    pred = model.predict(input_data)[0]
    proba = model.predict_proba(input_data)[0][pred]
    return f"{'🚨 Accident Likely' if pred else '✅ Safe'} (Confidence: {proba:.2f})"
# Interface
iface = gr.Interface(
    fn=predict_accident,
    inputs=[
        gr.Number(label="Number of Deaths"),
        gr.Number(label="Number of Injuries"),
        gr.Number(label="Speed Limit (km/h)"),
        gr.Number(label="Accident ID")
    ],
    outputs="text",
    title="AI Traffic Accident Predictor",
    description="Predict accident likelihood based on deaths, injuries, speed, and accident ID"
)
if __name__ == "__main__":
    iface.launch()
